---
layout: post
title: Note-PUMA
date: 2021-12-10
categories: Note
tags: [Papers, Note]
description: note taken when reading
---
# PUMA: A Programmable Ultra-efficient Memristor-based Accelerator for Machine Learning Inference

**[*Link*](https://arxiv.org/abs/1901.10351)**

**Reasons**

- Common design supports one or two types of NN, layers are encoded as state machines
  - not scalable to large workloads--increasing decoding overhead and complexity
- existing acc lack general ML workloads operations
  - LSTM nsot supported by xbars
- existing designs do not provide flexible data movement and control operation
  - to capture access variety and reuse patterns in different workloads

---

## Workload Characterization

### Multi-layer Perceptron

#### Dominance of MVM

```
mvm is the dominant ops in MLPs (suitable for xbar)
```

#### High data parallelism

- practical model sizes are larger than the typical on-chip storage (SRAM)
  - CMOS suffer from DRAM accesses, taxing because absence of data reuse to amortize
- xbars have high area efficiency allowing deploying many on one chip
- capture high data parallelism
- provides high storage density and bypass DRAM accesses

#### nonlinear operations

- implication on the architecture
  - consume more area than xbars
  - to size the units to provide throughput without offsetting xbar area/energy

### LSTM

- two MVMs by multiple (typically 4) vector and (typically 4) nonlinear functions

#### linear and transcendental ops

#### **Weight reuse**

- data reuse in LSTM
- benefits CMOS by amortizing DRAM accesses but not advantageous to xbars
- reuse only in a few inputs, workload remains memory-bound

### CNN

#### Input reuse and compute intensity

- map to matrix-matrix multiplications
- compute-bound and well-suited for CMOS hardware
- enough data to amortize DRAM access cost
- also perform well when MVMs
- implication:
  - minimize data movement
  - iterating over input, control flow  to represent the workload compactly without code bloat

#### Non-sequential access

- inputs are traversed and behavior of non-convolutional layers (pooling and response-normalization)
- implication
  - the need to support fine-grain/random access to memory
  - not needed for MLPs and LSTMs

---

## Core Architecture

- cores, tiles and nodes
- cores: analog xbars, functional units, instruction execution pipeline
- tiles: multiple cores connected via a shared memory
- nodes: multiple tiles connected via an on-chip network
- nodes can connected together via a chip-to-chip interconnect for large-scale execution

### instruction execution pipeline

- reason: existing use state machine--works well when scope of workloads is small
- proposed architecture:
  - break functionality down to finer-grain instructions
  - supplements memristor xbars with an instruction execution pipeline
- fetch, decode, execute
- instructions are 7 bytes wide

### matrix-vector multiplication unit (MVMU)

- the matrix is encoded as conductance states of the devices that constitute the xbar

#### precision considerations

- use 2 bits per device
- realize 16-bit MVM operations by combining 8 xbars via **bit-slicing**

#### xbar co-location and input sharing

- xbar peripherals higher area thatn xbar
- all eight 2-bit xbars of a 16-bit MVM operation are used simultaneously on the same input
- co-locate these 2-bit xbars on the same core in the same MVMU (*XbarIn* and DAC array to feed with minimal routing congestion)

#### Input shuffling

- reused input values may come at different positions in the input vector
- avoid moving data around
- MVM instruction provides operands

#### multiple MVMUs per core

- to activate multiple MVMUs in parallel (heavy operations)
- the ISA exposes and operand (*mask*) to allow a single MVM instruction to activate multiple at once

#### xbar writes

- puma--inference accelerator
- initialized with weights (serial writes) at configuration time prior to execution (not written to throughout execution)
- a spatial architecture--data is routed between xbars, each xbar stroes different portion of the model

### Vector Functional Unit (VFU)

- high data parallelism--wide vector operations--wide vector instructions
  - reducing instruction count
- narrow VFU because of hardware consideration (trade-off)
- a temporal SIMD (Single instruction, multiple data)
  - narrow width VFU execute wide vectors over multiple cycles
  - operands specify the starting address of vectors in the register file and vector width
  - operand *steer unit* holds the decoded instruction and reads the operands over subsequent cycles to feed VFU
  - additional *vecwidth* motivates wide instruction design

### Register File

- harboring general purpose registers
- providing area-efficient transcendental function evaluations

#### Implementing transcendental functions

- ROM-Embedded RAM structure (adds a wordline per row) (look up table)

#### sizing the register file

- majority of ML kernels--data is consumed with 1-2 instructions after being produced
- the compiler to reduce register pressure
- register file size 2(crossbar dimension)(#xbars per core)

#### ISA implications

- long operands--wide instruction design

### Memory Unit (MU)

- *load* and *store* instructions be executed at 16-bit word granularity
- *vec-width* operand for wide vector loads by sequential access patterns
- temporal SIMD to minimize fetch/decode energy consumption

### Static Instruction Usage

---

## Tile Architecture

- multiple cores connected to a shared memory (tile shared instruction memory)

### shared memory

- enabling inter-core synchronization
- sizing the shared memory to preserve storage density

#### inter-core synchronizationo
